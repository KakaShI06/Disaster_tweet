{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ujjval\n",
      "[nltk_data]     Priyadarshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk    \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "import re\n",
    "import string\n",
    "stopwords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt= pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b> Data Cleaning </b></h1>\n",
    "Here we are gonna clean the DF. Specifically, we clean:\n",
    "\n",
    "stopwords (Kept cause removing them cause drop of performances)\n",
    "<li>URL</li>\n",
    "<li>HTML</li>\n",
    "<li>emoji</li>\n",
    "<li>punctuation</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    def remove_stopwords(text):\n",
    "        if text is not None:\n",
    "            tokens = [x for x in word_tokenize(text) if x not in stop]\n",
    "            return \" \".join(tokens)\n",
    "        else:\n",
    "            return None\n",
    "       \n",
    "    df[\"text\"] = df['text'].apply(lambda x : x.lower())\n",
    "    \n",
    "    \n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url.sub(r'',text)\n",
    "    \n",
    "    df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "    \n",
    "    \n",
    "    def remove_html(text):\n",
    "        html=re.compile(r'<.*?>')\n",
    "        return html.sub(r'',text)\n",
    "    \n",
    "    df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "    \n",
    "    \n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "    \n",
    "    \n",
    "    def remove_punct(text):\n",
    "        table=str.maketrans('','',string.punctuation)\n",
    "        return text.translate(table)\n",
    "\n",
    "    df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "    \n",
    "    \n",
    "    df.text = df.text.replace('\\s+', ' ', regex=True)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = df[\"text\"].tolist()\n",
    "list_labels = df[\"target\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 6090)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/10\n",
      "6090/6090 [==============================] - 2s 345us/sample - loss: 0.6820 - acc: 0.5716 - val_loss: 0.6820 - val_acc: 0.5594\n",
      "Epoch 2/10\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.6711 - acc: 0.5731 - val_loss: 0.6639 - val_acc: 0.5594\n",
      "Epoch 3/10\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6262 - acc: 0.6415 - val_loss: 0.5862 - val_acc: 0.7544\n",
      "Epoch 4/10\n",
      "6090/6090 [==============================] - 1s 132us/sample - loss: 0.5133 - acc: 0.7990 - val_loss: 0.5060 - val_acc: 0.7663\n",
      "Epoch 5/10\n",
      "6090/6090 [==============================] - 1s 154us/sample - loss: 0.4185 - acc: 0.8356 - val_loss: 0.4535 - val_acc: 0.8063\n",
      "Epoch 6/10\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.3647 - acc: 0.8557 - val_loss: 0.4474 - val_acc: 0.8050\n",
      "Epoch 7/10\n",
      "6090/6090 [==============================] - 1s 152us/sample - loss: 0.3247 - acc: 0.8718 - val_loss: 0.4335 - val_acc: 0.8148\n",
      "Epoch 8/10\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.2927 - acc: 0.8867 - val_loss: 0.4308 - val_acc: 0.8142\n",
      "Epoch 9/10\n",
      "6090/6090 [==============================] - 1s 163us/sample - loss: 0.2676 - acc: 0.8984 - val_loss: 0.4337 - val_acc: 0.8148\n",
      "Epoch 10/10\n",
      "6090/6090 [==============================] - 1s 142us/sample - loss: 0.2415 - acc: 0.9099 - val_loss: 0.4413 - val_acc: 0.8155\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dp= model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for val in pred_dp:\n",
    "    if val > 0.5:\n",
    "        output.append(1)\n",
    "    else:\n",
    "        output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8154957321076822"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(output, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[736, 165],\n",
       "       [116, 506]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(output, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test = clean_df(dt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus= dt_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(test_corpus)\n",
    "padded_test = pad_sequences(sequences_test,maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "for val in pred:\n",
    "    if val > 0.5:\n",
    "        final_output.append(1)\n",
    "    else:\n",
    "        final_output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"id\": s[\"id\"],\n",
    "        \"target\": final_output\n",
    "    })\n",
    "\n",
    "submission.to_csv('submission_nlp.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
