{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ujjval\n",
      "[nltk_data]     Priyadarshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk    \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "import re\n",
    "import string\n",
    "stopwords= stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt= pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b> Data Cleaning </b></h1>\n",
    "Here we are gonna clean the DF. Specifically, we clean:\n",
    "\n",
    "stopwords (Kept cause removing them cause drop of performances)\n",
    "<li>URL</li>\n",
    "<li>HTML</li>\n",
    "<li>emoji</li>\n",
    "<li>punctuation</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    def remove_stopwords(text):\n",
    "        if text is not None:\n",
    "            tokens = [x for x in word_tokenize(text) if x not in stopwords]\n",
    "            return \" \".join(tokens)\n",
    "        else:\n",
    "            return None\n",
    "       \n",
    "    df[\"text\"] = df['text'].apply(lambda x : x.lower())\n",
    "    \n",
    "    df['text']= df['text'].apply(lambda x : remove_stopwords(x))\n",
    "    \n",
    "    \n",
    "    def remove_URL(text):\n",
    "        url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url.sub(r'',text)\n",
    "    \n",
    "    df['text']=df['text'].apply(lambda x : remove_URL(x))\n",
    "    \n",
    "    \n",
    "    def remove_html(text):\n",
    "        html=re.compile(r'<.*?>')\n",
    "        return html.sub(r'',text)\n",
    "    \n",
    "    df['text']=df['text'].apply(lambda x : remove_html(x))\n",
    "    \n",
    "    \n",
    "    def remove_emoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', text)\n",
    "\n",
    "    df['text']=df['text'].apply(lambda x: remove_emoji(x))\n",
    "    \n",
    "    \n",
    "    def remove_punct(text):\n",
    "        table=str.maketrans('','',string.punctuation)\n",
    "        return text.translate(table)\n",
    "\n",
    "    df['text']=df['text'].apply(lambda x : remove_punct(x))\n",
    "    \n",
    "    \n",
    "    df.text = df.text.replace('\\s+', ' ', regex=True)\n",
    "    return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_df(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_corpus = df[\"text\"].tolist()\n",
    "list_labels = df[\"target\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "sequences = tokenizer.texts_to_sequences(X_train)\n",
    "padded = pad_sequences(sequences,maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow_core\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\ML\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 24)                408       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 160,433\n",
      "Trainable params: 160,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/10\n",
      "6090/6090 [==============================] - 2s 369us/sample - loss: 0.6843 - acc: 0.5690 - val_loss: 0.6836 - val_acc: 0.5594\n",
      "Epoch 2/10\n",
      "6090/6090 [==============================] - 1s 118us/sample - loss: 0.6769 - acc: 0.5731 - val_loss: 0.6770 - val_acc: 0.5594\n",
      "Epoch 3/10\n",
      "6090/6090 [==============================] - 1s 140us/sample - loss: 0.6486 - acc: 0.6197 - val_loss: 0.6194 - val_acc: 0.6737\n",
      "Epoch 4/10\n",
      "6090/6090 [==============================] - 1s 143us/sample - loss: 0.5399 - acc: 0.7829 - val_loss: 0.5120 - val_acc: 0.7781\n",
      "Epoch 5/10\n",
      "6090/6090 [==============================] - 1s 139us/sample - loss: 0.4280 - acc: 0.8291 - val_loss: 0.4620 - val_acc: 0.7938\n",
      "Epoch 6/10\n",
      "6090/6090 [==============================] - 1s 127us/sample - loss: 0.3647 - acc: 0.8527 - val_loss: 0.4418 - val_acc: 0.8017\n",
      "Epoch 7/10\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.3213 - acc: 0.8736 - val_loss: 0.4294 - val_acc: 0.8129\n",
      "Epoch 8/10\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.2887 - acc: 0.8805 - val_loss: 0.4277 - val_acc: 0.8096\n",
      "Epoch 9/10\n",
      "6090/6090 [==============================] - 1s 122us/sample - loss: 0.2632 - acc: 0.8966 - val_loss: 0.4369 - val_acc: 0.8024\n",
      "Epoch 10/10\n",
      "6090/6090 [==============================] - 1s 120us/sample - loss: 0.2406 - acc: 0.9036 - val_loss: 0.4365 - val_acc: 0.8148\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(padded, y_train, epochs=num_epochs, validation_data=(testing_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dp= model.predict(testing_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for val in pred_dp:\n",
    "    if val > 0.5:\n",
    "        output.append(1)\n",
    "    else:\n",
    "        output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81483913328956"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(output, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[747, 177],\n",
       "       [105, 494]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(output, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test = clean_df(dt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus= dt_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(test_corpus)\n",
    "padded_test = pad_sequences(sequences_test,maxlen=max_length, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred= model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output = []\n",
    "for val in pred:\n",
    "    if val > 0.5:\n",
    "        final_output.append(1)\n",
    "    else:\n",
    "        final_output.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s= pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"id\": s[\"id\"],\n",
    "        \"target\": final_output\n",
    "    })\n",
    "\n",
    "submission.to_csv('submission_nlp.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
